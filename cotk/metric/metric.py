r"""
``cotk.metrics`` provides classes and functions evaluating results of models.
It provides a fair metric for every model.
"""
import random
from itertools import chain
import multiprocessing
from multiprocessing import Pool
import numpy as np
import tqdm
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
from .._utils.unordered_hash import UnorderedSha256
from .._utils.imports import DummyObject
from .._utils.metaclass import LoadClassInterface, DocStringInheritor
try:
	import torch
except ImportError as err:
	torch = DummyObject(err)
	torch.Tensor = DummyObject(err)

class MetricBase(LoadClassInterface, metaclass=DocStringInheritor):
	'''Base class for metrics.
	'''

	DATALOADER_ARGUMENTS = \
		r"""dataloader (:class:`.dataloader.GenerationBase`): A language generation dataloader."""
	REFERENCE_ALLVOCABS_KEY_ARGUMENTS = \
		r"""reference_allvocabs_key (str):
			The key of reference sentences. Default: ``ref_allvocabs``."""
	FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS = \
				r"""* **data[reference_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int. Reference sentences with allvocabs in index form.
				  Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				  Size: ``[batch_size, ~ref_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""
	FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("list or :class:`numpy.ndarray`", \
			"list or :class:`numpy.ndarray` or :class:`torch.Tensor`")
	FORWARD_POST_ALLVOCABS_ARGUMENTS = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"post_allvocabs_key")
	FORWARD_RESP_ALLVOCABS_ARGUMENTS = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"resp_allvocabs_key")

	MULTI_TURN_REFERENCE_ALLVOCABS_KEY_ARGUMENTS = \
		r"""multi_turn_reference_allvocabs_key (str):
			The key of reference sentences. Default: ``multi_turn_ref_allvocabs``."""
	FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS = \
				r"""* **data[multi_turn_reference_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array of int. Multi-turn reference sentences with all vocabs
				  with all vocabs. Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				  Size: ``[batch_size, ~turn_length, ~sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""
	FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH = \
		FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS.replace("list or :class:`numpy.ndarray`", \
			"list or :class:`numpy.ndarray` or :class:`torch.Tensor`")
	FORWARD_MULTI_TURN_CONTEXT_ALLVOCABS_ARGUMENTS = \
		FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"context_allvocabs_key")

	REFERENCE_LEN_KEY_ARGUMENTS = \
		r"""reference_len_key (str):
			The key of lengths of reference sentences.
			Default: ``ref_length``."""
	FORWARD_REFERENCE_LEN_ARGUMENTS = \
				r"""* **data[reference_len_key]** (list or :class:`numpy.ndarray`):
				  Length of reference sentences. Contains start token (eg:``<go>``)
				  and end token (eg:``<eos>``). Size: ``[batch_size]``."""

	MULTI_TURN_REFERENCE_LEN_KEY_ARGUMENTS = \
		r"""multi_turn_reference_len_key (str):
			The key of lengths of reference sentences.
			Default: ``multi_turn_ref_length``."""
	FORWARD_MULTI_TURN_REFERENCE_LEN_ARGUMENTS = \
				r"""* **data[multi_turn_reference_len_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int. **If padded, redundant position must be set to** ``0``.
				  Length of multi-turn reference sentences. Contains start token (eg:``<go>``)
				  and end token (eg:``<eos>``). Size: ``[batch_size, ~turn_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	GEN_KEY_ARGUMENTS = \
		r"""gen_key (str):
			The key of generated sentences. Default: ``gen``."""
	FORWARD_GEN_ARGUMENTS = \
				r"""* **data[gen_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int.
				  Sentences generated by model. Contains end token (eg: ``<eos>``),
				  but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	MULTI_TURN_GEN_KEY_ARGUMENTS = \
		r"""multi_turn_gen_key (str):
			The key of generated sentences. Default: ``multi_turn_gen``."""
	FORWARD_MULTI_TURN_GEN_ARGUMENTS = \
				r"""* **data[gen_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array of int. Sentences generated by model.
				  Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~max_turn_length, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	MULTI_TURN_LENGTH_KEY_ARGUMENTS = \
		r"""turn_length (str):
			The key of length of turns. Default: ``turn_length``."""
	FORWARD_MULTI_TURN_LENGTH_ARGUMENTS = \
				r"""* **data[turn_len_key]** (list or :class:`numpy.ndarray`):
				  Length of turns in each sample.
				  Size: ``[batch_size]``."""

	def __init__(self):
		self.unordered_hash = UnorderedSha256()
		self.closed = False

	def _hash_relevant_data(self, data_list):
		'''Invoked by :meth:`.forward` or :meth:`.close` to hash relevant data when computing a metric.

		Arguments:
			data_list (list): relevant data organized as list.
		'''
		for item in data_list:
			self.unordered_hash.update_data(repr(item).encode())

	def _hashvalue(self):
		'''Invoked by :meth:`.close` to return the recorded hash value.
		'''
		return self.unordered_hash.digest()

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict contains the data that metrics need.
		'''
		if self.closed:
			raise ValueError("The metric has been closed.")
		if not isinstance(data, dict):
			raise TypeError("Data must be a dict.")

	def close(self):
		'''
		Close the metric and return results. Once the metric is closed,
		any operation on the metric (e.g. forward or another close) will raise a ValueError.

		Returns:
			(dict) which contains results.
		'''
		if not self.closed:
			self.closed = True
			return {}
		else:
			raise RuntimeError("The metric has been closed.")

class _PrecisionRecallMetric(MetricBase):
	r"""Base class for precision recall metrics. This is an abstract class.

	Arguments:
		{ARGUMENTS}
	Attributes:
		res_prefix (str): Prefix added to the front of each key
					in the result dict of `close`.
	"""

	ARGUMENTS = r"""
		{MetricBase.DATALOADER_ARGUMENTS}
		generated_num_per_context (int): The number of sentences generated per context.
		candidate_allvocabs_key (str): The key of reference sentences. Default: ``candidate_allvocabs``.
		multiple_gen_key (str):
			The key of multiple generated sentences. Default: ``multiple_gen``."""


	def __init__(self, dataloader, \
				 generated_num_per_context, \
				 candidate_allvocabs_key='candidate_allvocabs', \
				 multiple_gen_key='multiple_gen'):
		super().__init__()
		self.dataloader = dataloader
		self.candidate_allvocabs_key = candidate_allvocabs_key
		self.multiple_gen_key = multiple_gen_key
		self.generated_num_per_context = generated_num_per_context
		self.prec_list = []
		self.rec_list = []
		self.res_prefix = ""

	def _score(self, gen, reference):
		r'''This function is called by :func:`forward`.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			int: score \in [0, 1].
		'''
		raise NotImplementedError( \
			"This function should be implemented by subclasses.")

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* **data[candidate_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged list of index. Multiple reference sentences for a single context.
				  Does not contain start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				  Size: ``[batch_size, ~sentence_num, ~word_num]``, where "~" means different sizes
				  in this dimension is allowed.
				* **data[multiple_gen_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array.
				  Sentences generated by model. Contains end token (eg: ``<eos>``),
				  but without start token (eg: ``<go>``).
				  Size: ``[batch_size, generated_num_per_context, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed.
		'''
		super().forward(data)
		candidate_allvocabs = data[self.candidate_allvocabs_key]
		multiple_gen = data[self.multiple_gen_key]

		if not isinstance(candidate_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for candidate_allvocabs.")
		if not isinstance(multiple_gen, (np.ndarray, list)):
			raise TypeError("Unknown type for multiple_gen")

		references = [[self.dataloader.trim_index(cand[1:]) for cand in inst] \
					  for inst in candidate_allvocabs]
		gens = [[self.dataloader.trim_index(cand) for cand in inst] \
					  for inst in multiple_gen]

		if len(references) != len(gens):
			raise ValueError("Batch num is not matched.")

		for line in gens:
			if len(line) != self.generated_num_per_context:
				raise ValueError(\
					"Number of geneated sentences per context does not equal to\
					the specified `generated_num_per_context`")

		self._hash_relevant_data(list(chain(*references)))
		for reference, gen in zip(references, gens):
			# pylint: disable=no-member
			matrix = np.zeros((len(reference), len(gen)), dtype=np.float32)
			for i, single_ref in enumerate(reference):
				for j, single_gen in enumerate(gen):
					matrix[i][j] = self._score(single_gen, single_ref)
			self.prec_list.append(float(np.sum(np.max(matrix, 0))) / len(gen))
			self.rec_list.append(float(np.sum(np.max(matrix, 1))) / len(reference))

	def close(self):
		'''
		Returns:
			(dict) returns a dict which contains

			* ``res_prefix`` **precision**: average precision.
			* ``res_prefix`` **recall**: average recall.
			* ``res_prefix`` **hashvalue**: hash value for precision & recall metric, same hash value stands
			  for same evaluation settings.
		'''
		res = super().close()
		res.update({'{} precision'.format(self.res_prefix): np.average(self.prec_list), \
				'{} recall'.format(self.res_prefix): np.average(self.rec_list), \
				'{} hashvalue'.format(self.res_prefix): self._hashvalue()})
		return res

class BleuPrecisionRecallMetric(_PrecisionRecallMetric):
	r'''Metric for calculating sentence BLEU precision and recall.

	References:
		[1] Zhao, T., Zhao, R., & Eskenazi, M. (2017). Learning discourse-level diversity
		for neural dialog models using conditional variational autoencoders.
		arXiv preprint arXiv:1703.10960.

	Arguments:
		{_PrecisionRecallMetric.ARGUMENTS}
		ngram (int): Specifies using BLEU-ngram.
	'''

	def __init__(self, dataloader, \
				 ngram, \
				 generated_num_per_context, \
				 candidates_allvocabs_key='candidate_allvocabs', \
				 multiple_gen_key='multiple_gen'):
		super().__init__(dataloader, generated_num_per_context, candidates_allvocabs_key, \
				multiple_gen_key)
		if ngram not in range(1, 5):
			raise ValueError("ngram should belong to [1, 4]")
		self.ngram = ngram
		self.weights = [1 / ngram] * ngram
		self.res_prefix = 'BLEU-{}'.format(ngram)
		self._hash_relevant_data([ngram, generated_num_per_context])

	def _score(self, gen, reference):
		r'''Score function of BLEU-ngram precision and recall.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			int: score \in [0, 1].
		'''
		return sentence_bleu([reference], gen, self.weights, SmoothingFunction().method1)

class EmbSimilarityPrecisionRecallMetric(_PrecisionRecallMetric):
	r'''Metric for calculating cosine similarity precision and recall.

	References:
		[1] Zhao, T., Zhao, R., & Eskenazi, M. (2017). Learning discourse-level diversity
		for neural dialog models using conditional variational autoencoders.
		arXiv preprint arXiv:1703.10960.

	Arguments:
		{_PrecisionRecallMetric.ARGUMENTS}
		word2vec (dict): Maps a word (str) to its pretrained embedding (:class:`numpy.ndarray` or list)
		mode (str): Specifies the operation that computes the bag-of-word representation.
			Must be ``avg`` or ``extrema``:

			* ``avg`` : element-wise average word embeddings.
			* ``extrema`` : element-wise maximum word embeddings.

	'''

	def __init__(self, dataloader, \
				 word2vec, \
				 mode, \
				 generated_num_per_context, \
				 candidates_allvocabs_key='candidate_allvocabs', \
				 multiple_gen_key='multiple_gen'):
		super().__init__(dataloader, generated_num_per_context, \
			candidates_allvocabs_key, multiple_gen_key)
		if not isinstance(word2vec, dict):
			raise ValueError("word2vec has invalid type")
		if word2vec:
			embed_shape = np.array(list(word2vec.values())).shape
			if len(embed_shape) != 2 or embed_shape[1] == 0:
				raise ValueError("word embeddings have inconsistent embedding size or are empty")
		if mode not in ['avg', 'extrema']:
			raise ValueError("mode should be 'avg' or 'extrema'.")
		self.word2vec = word2vec
		self.mode = mode
		self.res_prefix = '{}-bow'.format(mode)
		self._hash_relevant_data([mode, generated_num_per_context] + \
				[(word, list(emb)) for word, emb in self.word2vec.items()])

	def _score(self, gen, reference):
		r'''Score function of cosine similarity precision and recall.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			int: cosine similarity between two sentence embeddings \in [0, 1].
		'''
		gen_vec = []
		ref_vec = []
		for word in self.dataloader.convert_ids_to_tokens(gen):
			if word in self.word2vec:
				gen_vec.append(self.word2vec[word])
		for word in self.dataloader.convert_ids_to_tokens(reference):
			if word in self.word2vec:
				ref_vec.append(self.word2vec[word])
		if not gen_vec or not ref_vec:
			return 0
		if self.mode == 'avg':
			gen_embed = np.average(gen_vec, 0)
			ref_embed = np.average(ref_vec, 0)
		else:
			gen_embed = np.max(gen_vec, 0)
			ref_embed = np.max(ref_vec, 0)
		cos = np.sum(gen_embed * ref_embed) / \
			  np.sqrt(np.sum(gen_embed * gen_embed) * np.sum(ref_embed * ref_embed))
		norm = (cos + 1) / 2
		return norm

class PerplexityMetric(MetricBase):
	'''Metric for calculating perplexity.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.REFERENCE_ALLVOCABS_KEY_ARGUMENTS}
		{MetricBase.REFERENCE_LEN_KEY_ARGUMENTS}
		gen_log_prob_key (str): The key of **log** probability over words.
			Default: ``gen_log_prob``.
		invalid_vocab (bool): Whether ``gen_log_prob`` contains invalid vocab. Default: ``False``.
		full_check (bool): Whether perform a full check on ``gen_log_prob`` to make sure the sum
			of probability is 1. Otherwise, a random check will be performed for efficiency.
			If pytorch is used, a full check is always performed and this argument will be ignored.
			Default: ``False``.
	'''

	def __init__(self, dataloader, \
					   reference_allvocabs_key="ref_allvocabs", \
					   reference_len_key="ref_length", \
					   gen_log_prob_key="gen_log_prob", \
					   invalid_vocab=False, \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_allvocabs_key = reference_allvocabs_key
		self.reference_len_key = reference_len_key
		self.gen_log_prob_key = gen_log_prob_key
		self.word_loss = 0
		self.length_sum = 0
		self.invalid_vocab = invalid_vocab
		self.full_check = full_check
		self.engine_version = "unknown" # can be 'default', 'pytorch' when first forward time

		self.resp = []
		#self.resp_length = []
		self.gen_valid_log_prob = []
		self.gen_unk_log_prob = []

	def forward(self, data):
		'''Processing a batch of data. Smoothing will be performed for invalid vocabs.
		Unknowns vocabs will be ignored.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH}
				{MetricBase.FORWARD_REFERENCE_LEN_ARGUMENTS}
				* **data[gen_log_prob_key]** (list or :class:`numpy.ndarray` or :class:`torch.Tensor`):
				  Sentence generations model outputs of
				  A 3-d jagged or padded array of int. **log softmax** probability.
				  Contains end token (eg:``<eos>``), but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~gen_sentence_length, vocab_size]`` for ``invalid_vocab = False``, or
				  ``[batch_size, ~gen_sentence_length, all_vocab_size]`` for ``invalid_vocab = True``,
				  where "~" means different sizes in this dimension is allowed.
				  If :class:`torch.Tensor` is used, the following data should also be
				  :class:`torch.Tensor`.

		Warning:
			``data[gen_log_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(gen_log_prob), -1)`` equals ``np.ones((batch_size, gen_sentence_length))``
		'''
		super().forward(data)
		resp_allvocabs = data[self.reference_allvocabs_key]
		resp_length = data[self.reference_len_key]
		gen_log_prob = data[self.gen_log_prob_key]

		if not isinstance(resp_allvocabs, (torch.Tensor, np.ndarray, list)):
			raise TypeError("Unknown type for resp_allvocabs.")
		if not isinstance(gen_log_prob, (torch.Tensor, np.ndarray, list)):
			raise TypeError("Unknown type for gen_log_prob")
		if not isinstance(resp_length, (list, np.ndarray)):
			raise TypeError("Unknown type for resp_length")

		if self.engine_version == "unknown":
			if isinstance(gen_log_prob, torch.Tensor):
				self.engine_version = "pytorch"
			else:
				self.engine_version = "normal"

		if (self.engine_version == "pytorch") != isinstance(gen_log_prob, torch.Tensor):
			raise TypeError("If you want to use pytorch, `gen_log_prob` \
				should always be torch.Tensor. It can't mix with list or numpy.ndarray.")

		if self.engine_version == "pytorch":
			if not isinstance(resp_allvocabs, torch.Tensor):
				resp_allvocabs = gen_log_prob.new_tensor(resp_allvocabs)
			with torch.no_grad():
				self._pytorch_forward(resp_allvocabs, resp_length, gen_log_prob)
		else:
			self._normal_forward(resp_allvocabs, resp_length, gen_log_prob)

	def _normal_forward(self, resp_allvocabs, resp_length, gen_log_prob):
		if len(resp_allvocabs) != len(resp_length) or len(resp_allvocabs) != len(gen_log_prob):
			raise ValueError("Batch num of arguments is not matched.")

		# perform random check to assert the probability is valid
		checkid = random.randint(0, len(resp_length)-1)
		if resp_length[checkid] < 2:
			raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")
		checkrow = random.randint(0, resp_length[checkid]-2)

		random_check_expsum = np.sum(np.exp(gen_log_prob[checkid][checkrow]))
		if not np.isclose(random_check_expsum, 1):
			raise ValueError("data[gen_log_prob_key] must be processed after log_softmax. \
				gen_log_prob[%d][%d] exp sum is equal to %f." % (checkid, checkrow, \
				random_check_expsum))

		relevant_data = []
		for i, resp_len in enumerate(resp_length):
			if resp_len < 2:
				raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")

			resp_now = np.array(resp_allvocabs[i][1:resp_len])
			gen_now = np.array(gen_log_prob[i])
			relevant_data.append(resp_now.tolist())

			if len(resp_now.shape) != 1:
				raise ValueError("resp_allvocabs need to be 2 dimension")
			if len(gen_now.shape) != 2:
				raise ValueError("gen_log_prob need to be 3 dimension")

			# perform full check to assert the probability is valid
			if self.full_check:
				expsum = np.sum(np.exp(gen_now[:resp_len-1]), -1)
				if not np.allclose(expsum, [1] * (resp_len - 1)):
					raise ValueError("data[gen_log_prob_key] must be processed after log_softmax.")

			if not self.invalid_vocab:
				if gen_now.shape[1] != self.dataloader.vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to vocab_size when \
						invalid_vocab = False, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.vocab_size))
			else:
				if gen_now.shape[1] != self.dataloader.all_vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to all_vocab_size \
						when invalid_vocab = True, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.all_vocab_size))

			resp = resp_now
			self.resp.append(resp)
			#self.resp_length.append(resp_len)

			resp_known = resp.copy()
			if not self.invalid_vocab:
				resp_known[resp_known >= self.dataloader.vocab_size] = self.dataloader.unk_id

			self.gen_valid_log_prob.append(gen_now[list(range(resp_len-1)), resp_known])
			self.gen_unk_log_prob.append(gen_now[:resp_len-1, self.dataloader.unk_id])

		self._hash_relevant_data(relevant_data)

	def _pytorch_forward(self, resp_allvocabs, resp_length, gen_log_prob):
		if len(resp_allvocabs) != len(resp_length) or len(resp_allvocabs) != len(gen_log_prob):
			raise ValueError("Batch num of arguments is not matched.")
		if len(resp_allvocabs.shape) != 2:
			raise ValueError("resp_allvocabs need to be 2 dimension")
		if len(gen_log_prob.shape) != 3:
			raise ValueError("gen_log_prob need to be 3 dimension")

		relevant_data = []
		for i, resp_len in enumerate(resp_length):
			if resp_len < 2:
				raise ValueError("resp_length must no less than 2, because <go> and <eos> are always included.")

			resp_now = resp_allvocabs[i, 1:resp_len]
			gen_now = gen_log_prob[i, :resp_len - 1]
			relevant_data.append(resp_now.tolist())

			# perform full check to assert the probability is valid
			expsum = gen_now.exp().sum(-1)
			if not expsum.allclose(torch.ones_like(expsum)):
				raise ValueError("data[gen_log_prob_key] must be processed after log_softmax.")

			if not self.invalid_vocab:
				if gen_now.shape[1] != self.dataloader.vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to vocab_size when \
						invalid_vocab = False, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.vocab_size))
			else:
				if gen_now.shape[1] != self.dataloader.all_vocab_size:
					raise ValueError("The third dimension gen_log_prob should be equals to all_vocab_size \
						when invalid_vocab = True, \
						but %d != %d" % (gen_now.shape[1], self.dataloader.all_vocab_size))

			resp_known = resp_now.clone()
			if not self.invalid_vocab:
				resp_known[resp_known >= self.dataloader.vocab_size] = self.dataloader.unk_id

			unk_id = self.dataloader.unk_id
			vocab_size = self.dataloader.vocab_size
			invalid_vocab_size = self.dataloader.all_vocab_size - vocab_size

			# calc normal vocab
			normal_mask = ((resp_now != unk_id) & (resp_now < vocab_size)).float()
			word_loss = -(gen_now.gather(-1, resp_known.unsqueeze(1))[:, 0] * normal_mask).sum()
			length_sum = normal_mask.sum()
			# calc invalid vocab
			# smoothing from unk
			invalid_mask = (resp_now >= vocab_size).float()
			invalid_log_prob = (gen_now[:, unk_id] - \
						(torch.ones_like(gen_now[:, unk_id]) * invalid_vocab_size).log()) * invalid_mask

			if self.invalid_vocab:
				extra_invalid_log_prob = gen_now.gather(-1, resp_now.unsqueeze(1))[:, 0] * invalid_mask
				word_loss -= ((invalid_log_prob.exp() + extra_invalid_log_prob.exp()).log() \
						* invalid_mask).sum()
			else:
				word_loss -= invalid_log_prob.sum()

			length_sum += invalid_mask.sum()

			self.word_loss += word_loss.tolist()
			self.length_sum += length_sum.tolist()

		self._hash_relevant_data(relevant_data)

	@classmethod
	def _run_f(cls, ele):
		'''Auxiliary function for computing perplexity:

		Returns:

			* tuple: sum of log perplexity and sum of sentence length.
		'''
		valid_log_prob, unk_log_prob, resp_now, \
				invalid_vocab, vocab_size, all_vocab_size, unk_id = ele

		# calc normal vocab
		normal_idx = np.where(np.logical_and(resp_now != unk_id, \
								resp_now < vocab_size))
		word_loss = -np.sum(valid_log_prob[normal_idx])
		length_sum = np.array(normal_idx).shape[1]
		# calc invalid vocab
		# smoothing from unk
		invalid_idx = np.where(resp_now >= vocab_size)
		invalid_log_prob = unk_log_prob[invalid_idx] - np.log(all_vocab_size - vocab_size)
		if invalid_vocab:
			extra_invalid_log_prob = valid_log_prob[invalid_idx]
			word_loss -= np.sum(np.log( \
					np.exp(invalid_log_prob) + np.exp(extra_invalid_log_prob) \
				))
		else:
			word_loss -= np.sum(invalid_log_prob)
		length_sum += np.array(invalid_idx).shape[1]

		return word_loss, length_sum

	def close(self):
		r'''
		Returns:
			(dict): Return a dict which contains

			* **perplexity**: perplexity value.
			* **perplexity hashvalue**: hash value for perplexity metric, same hash value stands
			  for same evaluation settings.
		'''
		res = super().close()

		if self.engine_version == "pytorch":
			# pytorch is finished when forward
			pass
		else:
			loader = self.dataloader
			tasks = ((self.gen_valid_log_prob[i], self.gen_unk_log_prob[i], self.resp[i], \
							self.invalid_vocab, loader.vocab_size, loader.all_vocab_size, loader.unk_id) \
							for i, _ in enumerate(self.gen_valid_log_prob))

			# Multiprocessing seems can't boost the speed
			# if len(self.gen_valid_log_prob) > 100:
			# 	pool = Pool(multiprocessing.cpu_count())
			# 	for ans in tqdm.tqdm(pool.imap_unordered(self.run_f, tasks, chunksize=20), \
			# 		total=len(self.gen_valid_log_prob)):
			# 		self.word_loss += ans[0]
			# 		self.length_sum += ans[1]
			# 	pool.close()
			# 	pool.join()
			# else:
			for ans in map(self._run_f, tasks):
				self.word_loss += ans[0]
				self.length_sum += ans[1]

			self.resp = []
			self.gen_valid_log_prob = []
			self.gen_unk_log_prob = []

		print(self.word_loss)
		print(self.length_sum)

		res.update({"perplexity": np.exp(self.word_loss / self.length_sum), \
				"perplexity hashvalue": self._hashvalue()})
		return res

class MultiTurnPerplexityMetric(MetricBase):
	'''Metric for calculating multi-turn perplexity.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.MULTI_TURN_REFERENCE_ALLVOCABS_KEY_ARGUMENTS}
		{MetricBase.MULTI_TURN_REFERENCE_LEN_KEY_ARGUMENTS}
		gen_log_prob_key (str): The key of **log** probability over words.
			Default: ``multi_turn_gen_log_prob``.
		invalid_vocab (bool): whether ``gen_log_prob`` contains invalid vocab. Default: ``False``.
		full_check (bool): whether perform full checks on ``gen_log_prob`` to make sure the sum
			of probability is 1. Otherwise, a random check will be performed for efficiency.
			Default: ``False``.
	'''

	def __init__(self, dataloader, multi_turn_reference_allvocabs_key="multi_turn_ref_allvocabs", \
					   multi_turn_reference_len_key="multi_turn_ref_length", \
					   multi_turn_gen_log_prob_key="multi_turn_gen_log_prob", \
					   invalid_vocab=False, \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.multi_turn_reference_allvocabs_key = multi_turn_reference_allvocabs_key
		self.multi_turn_reference_len_key = multi_turn_reference_len_key
		self.multi_turn_gen_log_prob_key = multi_turn_gen_log_prob_key
		self.invalid_vocab = invalid_vocab
		self.sub_metric = PerplexityMetric(dataloader, \
				reference_allvocabs_key="ref_allvocabs", \
				reference_len_key="ref_length", \
				gen_log_prob_key="gen_log_prob", \
				invalid_vocab=invalid_vocab, \
				full_check=full_check)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH}
				{MetricBase.FORWARD_MULTI_TURN_REFERENCE_LEN_ARGUMENTS}
				* **data[multi_turn_gen_log_prob_key]** (list or :class:`numpy.ndarray` or \
					:class:`torch.Tensor`):
				  Sentence generations model outputs of
				  A 4-d jagged or padded array. **log softmax** probability.
				  Contains end token (eg:``<eos>``), but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~gen_sentence_length, vocab_size]`` for ``invalid_vocab = False``, or
				  ``[batch_size, ~gen_sentence_length, all_vocab_size]` for ``invalid_vocab = True``,
				  where "~" means different sizes in this dimension is allowed.
				  If :class:`torch.Tensor` is used, the following data should also be
				  :class:`torch.Tensor`.
		Warning:
			``data[multi_turn_gen_log_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(multi_turn_gen_log_prob_key), -1)`` equals
			``np.ones((batch_size, gen_sentence_length))``
		'''
		super().forward(data)
		reference_allvocabs = data[self.multi_turn_reference_allvocabs_key]
		length = data[self.multi_turn_reference_len_key]
		gen_log_prob = data[self.multi_turn_gen_log_prob_key]

		if not isinstance(reference_allvocabs, (torch.Tensor, np.ndarray, list)):
			raise TypeError("Unknown type for reference_allvocabs.")
		if not isinstance(length, (np.ndarray, list)):
			raise TypeError("Unknown type for length")
		if not isinstance(gen_log_prob, (torch.Tensor, list, np.ndarray)):
			raise TypeError("Unknown type for gen_log_prob")

		if len(length) != len(reference_allvocabs) or len(length) != len(gen_log_prob):
			raise ValueError("Batch num is not matched.")

		for i, sent_length in enumerate(length):
			# Pass turn as batch for sub_metric, the result will be same.
			turn_length = sent_length.index(0) if 0 in sent_length else len(sent_length)
			if len(reference_allvocabs[i]) < turn_length or len(gen_log_prob[i]) < turn_length:
				raise ValueError("Turn num is not matched.")
			self.sub_metric.forward({"ref_allvocabs": reference_allvocabs[i][:turn_length], \
					"ref_length": sent_length, \
					"gen_log_prob": gen_log_prob[i][:turn_length]})

	def close(self):
		r'''
		Returns:
			(dict): Return a dict which contains

			* **perplexity**: perplexity value.
			* **perplexity hashvalue**: hash value for perplexity metric, same hash value stands
			  for same evaluation settings.
		'''
		res = super().close()
		res.update(self.sub_metric.close())
		return res

class BleuCorpusMetric(MetricBase):
	'''Metric for calculating BLEU.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.REFERENCE_ALLVOCABS_KEY_ARGUMENTS}
		{MetricBase.GEN_KEY_ARGUMENTS}
	'''

	def __init__(self, dataloader, ignore_smoothing_error=False,\
			reference_allvocabs_key="ref_allvocabs", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.ignore_smoothing_error = ignore_smoothing_error
		self.reference_allvocabs_key = reference_allvocabs_key
		self.gen_key = gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_GEN_ARGUMENTS}
		'''
		super().forward(data)
		gen = data[self.gen_key]
		resp = data[self.reference_allvocabs_key]

		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen.")
		if not isinstance(resp, (np.ndarray, list)):
			raise TypeError("Unknown type for resp")

		if len(resp) != len(gen):
			raise ValueError("Batch num is not matched.")

		relevant_data = []
		for gen_sen, resp_sen in zip(gen, resp):
			self.hyps.append(self.dataloader.trim_index(gen_sen))
			reference = list(self.dataloader.trim_index(resp_sen[1:]))
			relevant_data.append(reference)
			self.refs.append([reference])
		self._hash_relevant_data(relevant_data)

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **bleu**: bleu value.
			* **bleu hashvalue**: hash value for bleu metric, same hash value stands
			  for same evaluation settings.
		'''
		result = super().close()
		try:
			result.update({"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7), \
				"bleu hashvalue": self._hashvalue()})
		except ZeroDivisionError as _:
			if not self.ignore_smoothing_error:
				raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")
			result.update({"bleu": \
					0, \
					"bleu hashvalue": self._hashvalue()})
		return result

class SelfBleuCorpusMetric(MetricBase):
	r'''Metric for calculating Self-BLEU.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.GEN_KEY_ARGUMENTS}
		sample (int): Number of examples sampled from the generated sentences. Default: ``1000``.
		seed (int): random seed for sampling. Default: ``1229``.

	Warning:
		the calculation of ``hashvalue`` considers the actual sample size of hypotheses which
			will be less than ``sample`` if the size of hypotheses is smaller than ``sample``
	'''

	def __init__(self, dataloader, \
		gen_key="gen", \
		sample=1000, \
		seed=1229):
		super().__init__()
		self.dataloader = dataloader
		self.gen_key = gen_key
		self.sample = sample
		self.refs = []
		self.hyps = []
		self.seed = seed

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_GEN_ARGUMENTS}
		'''
		super().forward(data)
		gen = data[self.gen_key]

		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen.")

		for gen_sen in gen:
			self.hyps.append(self.dataloader.trim_index(gen_sen))

	def _run_f(self, ele):
		'''Auxiliary function for computing sentence bleu:

		Arguments:
			ele (tuple): A tuple (`reference sentences`, `a hypothesis sentence`).

		Returns:

			* int: **sentence-bleu** value.
		'''
		return sentence_bleu(ele[0], ele[1], smoothing_function=SmoothingFunction().method1)

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **self-bleu**: self-bleu value.
		'''
		res = super().close()

		if self.sample > len(self.hyps):
			self.sample = len(self.hyps)
		random.seed(self.seed)
		random.shuffle(self.hyps)
		ref = self.hyps[:self.sample]

		bleu_irl = []
		if self.sample >= 1000:
			tasks = ((ref[:i]+ref[i+1:self.sample], ref[i]) for i in range(self.sample))
			pool = Pool(multiprocessing.cpu_count())
			for ans in tqdm.tqdm(pool.imap_unordered(self._run_f, tasks, chunksize=20), total=self.sample):
				bleu_irl.append(ans)
			pool.close()
			pool.join()
		elif self.sample > 1:
			for i in range(self.sample):
				bleu_irl.append(self._run_f((ref[:i]+ref[i+1:], ref[i])))
		self._hash_relevant_data([self.seed, self.sample])
		res.update({"self-bleu" : 1.0 * sum(bleu_irl) / len(bleu_irl),\
					"self-bleu hashvalue": self._hashvalue()})
		return res

class FwBwBleuCorpusMetric(MetricBase):
	r'''Metric for calculating FwBw-BLEU.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		reference_test_key (str): Reference sentences with all vocabs in test data
			are passed to :func:`forward` by ``dataloader.data["test"][self.reference_test_key]``.
		{MetricBase.GEN_KEY_ARGUMENTS}
		sample (int): Number of examples sampled from the generated sentences. Default: ``1000``.
		seed (int): random seed for sampling. Default: ``1229``.

	Warning:
		The calculation of ``hashvalue`` considers the actual sample size of hypotheses and 
		references. Therefore ``hashvalue`` may vary with the size of hypothesis or references
		if the size of them is smaller than ``sample``.
	'''

	def __init__(self, dataloader, \
			reference_test_key, \
			gen_key="gen", \
			sample=1000, \
			seed=1229):
		super().__init__()
		self.dataloader = dataloader
		self.reference_test_key = reference_test_key
		self.gen_key = gen_key
		self.sample = sample
		self.seed = seed
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_GEN_ARGUMENTS}
		'''
		gen = data[self.gen_key]

		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen.")

		for gen_sen in gen:
			self.hyps.append(list(self.dataloader.trim_index(gen_sen)))

	def _run_f(self, ele):
		'''Auxiliary function for computing sentence bleu:

		Arguments:
			ele (tuple): A tuple (`reference sentences`, `a hypothesis sentence`).

		Returns:

			* int: **sentence-bleu** value.
		'''
		return sentence_bleu(ele[0], ele[1], smoothing_function=SmoothingFunction().method1)

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **fwbwbleu**: fw/bw bleu value.
			* **fw-bw-bleu hashvalue**: hash value for fwbwbleu metric, same hash value stands
			  for same evaluation settings.
		'''
		res = super().close()

		resp = self.dataloader.data["test"][self.reference_test_key]
		for resp_sen in resp:
			self.refs.append(list(self.dataloader.trim_index(resp_sen[1:])))

		sample_hyps = self.sample if self.sample < len(self.hyps) else len(self.hyps)
		sample_refs = self.sample if self.sample < len(self.refs) else len(self.refs)

		random.seed(self.seed)
		random.shuffle(self.hyps)
		random.shuffle(self.refs)

		bleu_irl_fw, bleu_irl_bw = [], []
		if sample_hyps >= 1000:
			tasks = ((self.refs, self.hyps[i]) for i in range(sample_hyps))
			pool = Pool(multiprocessing.cpu_count())
			for ans in tqdm.tqdm(pool.imap_unordered(self._run_f, tasks, chunksize=20), total=sample_hyps):
				bleu_irl_fw.append(ans)
			pool.close()
			pool.join()
		else:
			for i in range(sample_hyps):
				bleu_irl_fw.append(self._run_f((self.refs, self.hyps[i])))

		if sample_refs >= 1000:
			tasks = ((self.hyps, self.refs[i]) for i in range(sample_refs))
			pool = Pool(multiprocessing.cpu_count())
			for ans in tqdm.tqdm(pool.imap_unordered(self._run_f, tasks, chunksize=20), total=sample_refs):
				bleu_irl_bw.append(ans)
			pool.close()
			pool.join()
		else:
			for i in range(sample_refs):
				bleu_irl_bw.append(self._run_f((self.hyps, self.refs[i])))
		fw_bleu = (1.0 * sum(bleu_irl_fw) / len(bleu_irl_fw))
		bw_bleu = (1.0 * sum(bleu_irl_bw) / len(bleu_irl_bw))
		if fw_bleu + bw_bleu > 0:
			fw_bw_bleu = 2.0 * bw_bleu * fw_bleu / (fw_bleu + bw_bleu)
		else:
			fw_bw_bleu = 0

		res.update({"fw-bleu" : fw_bleu, \
			"bw-bleu" : bw_bleu, \
			"fw-bw-bleu" : fw_bw_bleu \
		})

		self._hash_relevant_data(self.refs + [self.seed, sample_hyps, sample_refs])
		res.update({"fw-bw-bleu hashvalue" : self._hashvalue()})
		return res

class MultiTurnBleuCorpusMetric(MetricBase):
	'''Metric for calculating multi-turn BLEU.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.MULTI_TURN_REFERENCE_ALLVOCABS_KEY_ARGUMENTS}
		{MetricBase.MULTI_TURN_GEN_KEY_ARGUMENTS}
		{MetricBase.MULTI_TURN_LENGTH_KEY_ARGUMENTS}
	'''
	def __init__(self, dataloader, ignore_smoothing_error=False,\
					multi_turn_reference_allvocabs_key="reference_allvocabs", \
					multi_turn_gen_key="multi_turn_gen", \
					turn_len_key="turn_length" \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.ignore_smoothing_error = ignore_smoothing_error
		self.multi_turn_reference_allvocabs_key = multi_turn_reference_allvocabs_key
		self.turn_len_key = turn_len_key
		self.multi_turn_gen_key = multi_turn_gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_MULTI_TURN_GEN_ARGUMENTS}
				{MetricBase.FORWARD_MULTI_TURN_LENGTH_ARGUMENTS}
		'''
		super().forward(data)
		reference_allvocabs = data[self.multi_turn_reference_allvocabs_key]
		length = data[self.turn_len_key]
		gen = data[self.multi_turn_gen_key]

		if not isinstance(reference_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for reference_allvocabs.")
		if not isinstance(length, (np.ndarray, list)):
			raise TypeError("Unknown type for length")
		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen")

		if len(length) != len(reference_allvocabs) or len(length) != len(gen):
			raise ValueError("Batch num is not matched.")

		for i, turn_length in enumerate(length):
			gen_session = gen[i]
			ref_session = reference_allvocabs[i]
			for j in range(turn_length):
				self.hyps.append(list(self.dataloader.trim_index(gen_session[j])))
				self.refs.append([list(self.dataloader.trim_index(ref_session[j])[1:])])

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **bleu**: bleu value.
			* **bleu hashvalue**: hash value for bleu metric, same hash value stands
			  for same evaluation settings.
		'''
		result = super().close()
		self._hash_relevant_data(self.refs)

		try:
			result.update({"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7), \
				"bleu hashvalue": self._hashvalue()})
		except ZeroDivisionError as _:
			if not self.ignore_smoothing_error:
				raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")
			result.update({"bleu": \
					0, \
					"bleu hashvalue": self._hashvalue()})
		return result

class SingleTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		post_allvocabs_key (str): The key of dialog posts with allvocabs.
			Default: ``post_allvocabs``.
		resp_allvocabs_key (str): The key of dialog responses with allvocabs.
			Default: ``resp_allvocabs``.
		{MetricBase.GEN_KEY_ARGUMENTS}
	'''
	def __init__(self, dataloader, post_allvocabs_key="post_allvocabs", \
			resp_allvocabs_key="resp_allvocabs", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.post_allvocabs_key = post_allvocabs_key
		self.resp_allvocabs_key = resp_allvocabs_key
		self.gen_key = gen_key
		self.post_list = []
		self.resp_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_POST_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_RESP_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_GEN_ARGUMENTS}
		'''
		super().forward(data)
		post_allvocabs = data[self.post_allvocabs_key]
		resp_allvocabs = data[self.resp_allvocabs_key]
		gen = data[self.gen_key]

		if not isinstance(post_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for post_allvocabs.")
		if not isinstance(resp_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for resp_allvocabs")
		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen")

		if len(post_allvocabs) != len(resp_allvocabs) or len(resp_allvocabs) != len(gen):
			raise ValueError("Batch num is not matched.")
		for i, post_sen in enumerate(post_allvocabs):
			self.post_list.append(self.dataloader.convert_ids_to_tokens(post_sen[1:]))
			self.resp_list.append(self.dataloader.convert_ids_to_tokens(resp_allvocabs[i][1:]))
			self.gen_list.append(self.dataloader.convert_ids_to_tokens(gen[i]))

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **post**: a list of post sentences. A jagged 2-d array of int.
			  Size:``[batch_size, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
			* **resp**: a list of response sentences. A jagged 2-d array of int.
			  Size:``[batch_size, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
			* **gen**: A list of generated sentences. A jagged 2-d array of int.
			  Size:``[batch_size, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
		'''
		res = super().close()
		res.update({"post": self.post_list, "resp": self.resp_list, "gen": self.gen_list})
		return res

class MultiTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		multi_turn_context_allvocabs_key (str): The key of dialog context with allvocabs.
			Default: ``multi_turn_context_allvocabs``.
		multi_turn_reference_allvocabs_key (str): The key of dialog references with allvocabs.
			Default: ``multi_turn_ref_allvocabs``.
		{MetricBase.MULTI_TURN_GEN_KEY_ARGUMENTS}
		{MetricBase.MULTI_TURN_LENGTH_KEY_ARGUMENTS}
	'''
	def __init__(self, dataloader, multi_turn_context_allvocabs_key="multi_turn_context_allvocabs", \
			multi_turn_reference_allvocabs_key="multi_turn_ref_allvocabs", \
			multi_turn_gen_key="multi_turn_gen", \
			turn_len_key="turn_length"):
		super().__init__()
		self.dataloader = dataloader
		self.multi_turn_context_allvocabs_key = multi_turn_context_allvocabs_key
		self.multi_turn_reference_allvocabs_key = multi_turn_reference_allvocabs_key
		self.multi_turn_gen_key = multi_turn_gen_key
		self.turn_len_key = turn_len_key
		self.context_list = []
		self.reference_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_MULTI_TURN_CONTEXT_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS}
				{MetricBase.FORWARD_MULTI_TURN_GEN_ARGUMENTS}
				{MetricBase.FORWARD_MULTI_TURN_LENGTH_ARGUMENTS}
		'''
		super().forward(data)
		context_allvocabs = data[self.multi_turn_context_allvocabs_key]
		reference_allvocabs = data[self.multi_turn_reference_allvocabs_key]
		gen = data[self.multi_turn_gen_key]
		turn_length = data[self.turn_len_key]

		if not isinstance(context_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for context_allvocabs.")
		if not isinstance(reference_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for reference_allvocabs")
		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen")
		if not isinstance(turn_length, (np.ndarray, list)):
			raise TypeError("Unknown type for turn_length")

		if len(turn_length) != len(context_allvocabs) or \
			len(turn_length) != len(reference_allvocabs) or \
			len(turn_length) != len(gen):
			raise ValueError("Batch num is not matched.")

		for i, context_sen in enumerate(context_allvocabs):
			self.context_list.append(self.dataloader.convert_multi_turn_ids_to_tokens( \
				np.array(context_sen), ignore_first_token=True))
			self.reference_list.append(self.dataloader.convert_multi_turn_ids_to_tokens( \
				np.array(reference_allvocabs[i]), turn_length=turn_length[i], ignore_first_token=True))
			self.gen_list.append(self.dataloader.convert_multi_turn_ids_to_tokens( \
				np.array(gen[i]), turn_length=turn_length[i]))
			print(turn_length[i])
			print(len(self.reference_list[-1]))

			if len(self.reference_list[-1]) != len(self.gen_list[-1]):
				raise ValueError("Reference turn num %d != gen turn num %d." % \
						(len(self.reference_list[-1]), len(self.gen_list[-1])))

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **context**: a list of post sentences. A jagged 3-d array of int.
			  Size:``[batch_size, ~turn_length, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
			* **reference**: a list of response sentences. A jagged 3-d array of int.
			  Size:``[batch_size, ~turn_length, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
			* **gen**: a list of generated sentences. A jagged 3-d array of int.
			  Size:``[batch_size, ~turn_length, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
		'''
		res = super().close()
		res.update({"context": self.context_list, "reference": self.reference_list, "gen": self.gen_list})
		return res

class LanguageGenerationRecorder(MetricBase):
	'''A metric-like class for recorder generated sentences.

	Arguments:
		{MetricBase.DATALOADER_ARGUMENTS}
		{MetricBase.GEN_KEY_ARGUMENTS}
	'''
	def __init__(self, dataloader, gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.gen_key = gen_key
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				{MetricBase.FORWARD_GEN_ARGUMENTS}
		'''
		super().forward(data)
		gen = data[self.gen_key]

		if not isinstance(gen, (np.ndarray, list)):
			raise TypeError("Unknown type for gen")

		for sen in gen:
			self.gen_list.append(self.dataloader.convert_ids_to_tokens(sen))

	def close(self):
		'''
		Returns:
			(dict): Return a dict which contains

			* **gen**: a list of generated sentences. A jagged 2-d array of int.
			  Size:``[batch_size, ~sent_length]``, where "~" means different
			  sizes in this dimension is allowed.
		'''
		res = super().close()
		res.update({"gen": self.gen_list})
		return res

class MetricChain(MetricBase):
	'''A metric-like class for stacked metric. You can use this class
	making multiples metric combination like one.

	Examples:
		>>> metric = MetricChain()
		>>> metric.add_metric(BleuCorpusMetric())
		>>> metric.add_metric(SingleDialogRecorder(dataloader))

	Todo: Give more examples to combining forward and close
	'''
	def __init__(self):
		super().__init__()
		self.metric_list = []

	def add_metric(self, metric):
		'''Add metric for processing.

		Arguments:
			metric (MetricBase): a metric class.
		'''
		if not isinstance(metric, MetricBase):
			raise TypeError("Metric must be a subclass of MetricBase")
		self.metric_list.append(metric)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains keys which all the
				metric components need.
		'''
		super().forward(data)
		for metric in self.metric_list:
			metric.forward(data)

	def close(self):
		r'''
		Returns:
			(dict): A dict which contains the items which all the
			metric components returned.
		'''
		res = super().close()
		for metric in self.metric_list:
			res.update(metric.close())
		return res
